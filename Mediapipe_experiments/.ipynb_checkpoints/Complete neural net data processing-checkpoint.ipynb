{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194cbc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Points import Points\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d2e6d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {\"anger\": 0, \"contempt\": 1, \"disgust\": 2, \"fear\": 3, \"happiness\": 4, \"neutral\": 5, \"sadness\": 6,\"sad\": 6,\n",
    "            \"surprise\": 6}\n",
    "\n",
    "def read_data(path, label_points):\n",
    "    '''Path for json with '''\n",
    "    landmarks = pd.read_json(path)\n",
    "    emotions = np.asarray([emotion_dict[y] for y in landmarks.iloc[0].astype(object)], dtype=np.float32)\n",
    "    columns = landmarks.shape[1]\n",
    "    points = []\n",
    "    for photo in range(columns):\n",
    "        for point in label_points:\n",
    "            coords = landmarks[photo].iloc[point+1][0:2]\n",
    "            points.extend(coords)\n",
    "\n",
    "    points = np.array(points, dtype=np.float32).reshape(1133, len(label_points*2))\n",
    "\n",
    "    print(f\"shape at the end: {points.shape}\")\n",
    "    return emotions, points\n",
    "    #return [[a, b] for (a, b) in zip(np.concatenate((JAFFE_EMO, CK_EMO)), np.concatenate((JAFFE_VEC, CK_VEC)))] #\n",
    "    \n",
    "def select_points(emotions, points, training_ratio):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(emotions, points, \n",
    "                                                        test_size=(1-training_ratio), random_state=1234)\n",
    "    \n",
    "    X_train = torch.from_numpy(X_train)\n",
    "    X_test = torch.from_numpy(X_test)\n",
    "    \n",
    "    new_y_train = np.zeros([y_train.shape[0], 7])\n",
    "    for index in range(y_train.shape[0]):\n",
    "        print(y_train[index])\n",
    "        if int(y_train[index]) != emotion_dict[\"neutral\"]:\n",
    "            new_y_train[index][int(y_train[index])] = 10\n",
    "    \n",
    "    new_y_test = np.zeros([y_test.shape[0], 7])\n",
    "    for index in range(y_test.shape[0]):\n",
    "        if int(y_test[index]) != emotion_dict[\"neutral\"]:\n",
    "            new_y_test[index][int(y_test[index])] = 10\n",
    "        \n",
    "    \n",
    "    y_train = torch.from_numpy(new_y_train)\n",
    "    y_test = torch.from_numpy(new_y_test)\n",
    "    \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8fc9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Net model\n",
    "\n",
    "# 1) Model\n",
    "# Linear model f = wx + b , sigmoid at the end\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_input_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 268)\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        x = F.relu(self.fc2(x), inplace=True)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Model(268)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a97adac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape at the end: (1133, 268)\n",
      "[0.4057805  0.32506374 0.3928023  0.3010693  0.35257268 0.27402917\n",
      " 0.30962166 0.2611924  0.26190275 0.26016912 0.2179098  0.2690574\n",
      " 0.18971837 0.2850256  0.1715053  0.30385444 0.17447612 0.32785743\n",
      " 0.19226842 0.3451714  0.21339728 0.35390934 0.2480169  0.35769692\n",
      " 0.28909904 0.35830283 0.3314563  0.35446876 0.3682079  0.34584746\n",
      " 0.39185143 0.33688506 0.60090655 0.3237928  0.61104447 0.30002457\n",
      " 0.6497071  0.27351874 0.69128126 0.2599758  0.73832536 0.25728372\n",
      " 0.78243977 0.2641603  0.8105866  0.2789158  0.82857877 0.29750955\n",
      " 0.8263827  0.32133794 0.81090826 0.33746898 0.79158795 0.34550798\n",
      " 0.75891083 0.34895882 0.7185291  0.35040474 0.6765983  0.3487763\n",
      " 0.6400663  0.34233958 0.61624455 0.33485097 0.49692678 0.2801042\n",
      " 0.45950004 0.33148518 0.462412   0.36953223 0.43332702 0.4238765\n",
      " 0.40156913 0.44982073 0.371802   0.47060162 0.34012255 0.51492417\n",
      " 0.30962166 0.2611924  0.26190275 0.26016912 0.5055355  0.5695958\n",
      " 0.5695233  0.56427467 0.6450918  0.5477427  0.66812736 0.50989777\n",
      " 0.6332359  0.46659368 0.6019534  0.44706842 0.5686924  0.42189455\n",
      " 0.5366788  0.36843508 0.53760713 0.33054373 0.51124436 0.66961914\n",
      " 0.44442168 0.66557014 0.3888859  0.66357064 0.344852   0.6640562\n",
      " 0.30678755 0.6662267  0.27505177 0.66894895 0.31367013 0.70135665\n",
      " 0.34812686 0.72377884 0.39099765 0.743017   0.4458706  0.75613654\n",
      " 0.5109576  0.7608148  0.5769391  0.7541185  0.63223976 0.73943406\n",
      " 0.6754827  0.7192533  0.7096027  0.696564   0.7431737  0.66524285\n",
      " 0.7168007  0.66209495 0.6790556  0.6609356  0.6349111  0.66125494\n",
      " 0.5790398  0.6642065  0.3905407  0.32572442 0.37707064 0.31848285\n",
      " 0.3482921  0.30905083 0.31018674 0.30413532 0.27619398 0.30471134\n",
      " 0.2432138  0.310505   0.22170088 0.31755164 0.2086224  0.32278538\n",
      " 0.19818576 0.3268941  0.21453452 0.33160746 0.2313364  0.33409485\n",
      " 0.25463024 0.3363416  0.28929895 0.3370326  0.3241285  0.33439383\n",
      " 0.35792136 0.32993928 0.37978593 0.32776877 0.4261751  0.33080363\n",
      " 0.416798   0.2933843  0.38017783 0.2606359  0.30778638 0.23923017\n",
      " 0.2457623  0.23821676 0.19472553 0.2503059  0.15987632 0.27107\n",
      " 0.14442177 0.30202344 0.14146326 0.33706442 0.15275398 0.36635932\n",
      " 0.17870876 0.37868318 0.22373322 0.38532194 0.2814652  0.38469726\n",
      " 0.3355041  0.37654617 0.37801528 0.36455625 0.4060855  0.35202652\n",
      " 0.61657816 0.32420626 0.6303424  0.31660646 0.6580941  0.30657074\n",
      " 0.69383085 0.30003157 0.72705925 0.29876885 0.7602616  0.30309722\n",
      " 0.7817194  0.31012365 0.79471505 0.31565902 0.80449736 0.32010457\n",
      " 0.78903645 0.32363373 0.7731341  0.3253241  0.7513182  0.32733732\n",
      " 0.71776843 0.32886064 0.6833944  0.3279564  0.64977735 0.32597494\n",
      " 0.62779665 0.32562727 0.57862234 0.32884577 0.5845958  0.2915266\n",
      " 0.61841124 0.25936663 0.6892628  0.23694503 0.75147885 0.23385821\n",
      " 0.80309325 0.24408215 0.8387778  0.26410764 0.85441566 0.29472277\n",
      " 0.8577402  0.3291897  0.8502877  0.35651335 0.8269803  0.3687775\n",
      " 0.78499    0.3756004  0.7283229  0.37684155 0.6738151  0.3709858\n",
      " 0.63025343 0.36102948 0.60098076 0.3492513 ]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m max_wanted_points \u001b[38;5;241m=\u001b[39m min_wanted_points \u001b[38;5;241m+\u001b[39m Points\u001b[38;5;241m.\u001b[39mright_eye_inner\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m+\u001b[39m Points\u001b[38;5;241m.\u001b[39mright_eye_outer\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m      4\u001b[0m         Points\u001b[38;5;241m.\u001b[39mleft_eye_inner\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m+\u001b[39m Points\u001b[38;5;241m.\u001b[39mleft_eye_outer\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m      6\u001b[0m emotions, points \u001b[38;5;241m=\u001b[39m read_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./photo_landmark_list.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_wanted_points)\n\u001b[0;32m----> 7\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mselect_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43memotions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.95\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 2) Loss and optimizer\u001b[39;00m\n\u001b[1;32m     10\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mselect_points\u001b[0;34m(emotions, points, training_ratio)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(y_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(y_train[index])\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m!=\u001b[39m emotion_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     32\u001b[0m         new_y_train[index][\u001b[38;5;28mint\u001b[39m(y_train[index])] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     34\u001b[0m new_y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros([y_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m7\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# 1)\n",
    "min_wanted_points = Points.right_eye_middle.value + Points.left_eye_middle.value + Points.nose.value + Points.mouth_inner.value\n",
    "max_wanted_points = min_wanted_points + Points.right_eye_inner.value + Points.right_eye_outer.value + \\\n",
    "        Points.left_eye_inner.value + Points.left_eye_outer.value\n",
    "\n",
    "emotions, points = read_data(\"./photo_landmark_list.json\", max_wanted_points)\n",
    "X_train, X_test, y_train, y_test = select_points(emotions, points, .95)\n",
    "\n",
    "# 2) Loss and optimizer\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "counter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and loss\n",
    "    y_pred = model(X_train)\n",
    "    new_label = torch.from_numpy(y_train).float().view(1076,7)\n",
    "    loss = criterion(new_pred, new_label)\n",
    "\n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        y_predicted = model(X_test)\n",
    "        for row in range(y_predicted.shape[0]):\n",
    "            dist = 100\n",
    "            closest_index = None\n",
    "            for index in range(7):\n",
    "                if abs(float(y_predicted[row][index])-10) < dist:\n",
    "                    dist = abs(float(y_predicted[row][index])-10)\n",
    "                    closest_index = index\n",
    "            if closest_index == y_test[row]:\n",
    "                correct += 1\n",
    "\n",
    "        print(f\"correct: {correct}\")\n",
    "        print(f\"total: {y_predicted.shape[0]}\")\n",
    "\n",
    "#torch.save(model.state_dict(), \"./model-v10.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61645b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
