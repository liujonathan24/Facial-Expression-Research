{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch-Tutorial \n",
    "## Chapter - 1 (The Classification)\n",
    "![logo](https://code.kaytouch.biz/wp-content/uploads/2018/05/pytorch-470x250.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Pytorch tutorial is a series of tutorials created by me to explain the basic aspects of PyTorch and its implementation. PyTorch is complex to implement but not difficult. If you see it as a way of documentation or documenting a program, then things get much easier to understand. The most interesting part of this series is that I am also a beginner with PyTorch, so what's difficult me I expect to be difficult for five other individuals also, so at least I expect my tutorial could help five other individuals to implement PyTorch.<br>\n",
    "In this chapter, I have implemented an image classification problem with the help of PyTorch. Here I have explained everything in the most basic way possible so that you could also understand them easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "The things that are explained in this classification tutorial are given below.\n",
    "* Creating a custom dataset\n",
    "* Creating a neural network in PyTorch\n",
    "* Training neural network in PyTorch\n",
    "* Plotting of loss and accuracy curve\n",
    "* evaluation of performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-set\n",
    "Dataset used - [Arthropod Taxonomy Orders Object Detection Dataset](https://www.kaggle.com/mistag/arthropod-taxonomy-orders-object-detection-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \u001b[38;5;66;03m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm_notebook \u001b[38;5;28;01mas\u001b[39;00m tqdm\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "# Neural networks can be constructed using the torch.nn package.\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "'''for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))'''\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device selection\n",
    "Select your device, whether you to use CPU or GPU for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "BASE_PATH = '/kaggle/input/arthropod-taxonomy-orders-object-detection-dataset/ArTaxOr/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=[]\n",
    "labels=[]\n",
    "for file in os.listdir(BASE_PATH):\n",
    "    if file=='Coleoptera':\n",
    "        for c in os.listdir(os.path.join(BASE_PATH, file)):\n",
    "            if c!='annotations':\n",
    "                image.append(c)\n",
    "                labels.append('Coleoptera')\n",
    "    if file=='Diptera':\n",
    "        for c in os.listdir(os.path.join(BASE_PATH, file)):\n",
    "            if c!='annotations':\n",
    "                image.append(c)\n",
    "                labels.append('Diptera')\n",
    "    if file=='Hymenoptera':\n",
    "        for c in os.listdir(os.path.join(BASE_PATH, file)):\n",
    "            if c!='annotations':\n",
    "                image.append(c)\n",
    "                labels.append('Hymenoptera')\n",
    "    if file=='Lepidoptera':\n",
    "        for c in os.listdir(os.path.join(BASE_PATH, file)):\n",
    "            if c!='annotations':\n",
    "                image.append(c)\n",
    "                labels.append('Lepidoptera')\n",
    "data = {'Images':image, 'labels':labels} \n",
    "data = pd.DataFrame(data) \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** - In the cell above I have created a csv Data-frame from the raw dataset. In your case you might not have to follow this step if you are already provided with csv file which contains the desired input and target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelEncoder()\n",
    "data['encoded_labels'] = lb.fit_transform(data['labels'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting of Dataset\n",
    "I have shown two ways to split a dataset into train and validation. One is by splitting it from sratch and another method is by using train_test_split from scikit-learn( the one which I have commented out).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "validation_split = .3\n",
    "shuffle_dataset = True\n",
    "random_seed= 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data indices for training and validation splits:\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# tr, val = train_test_split(data.label, stratify=data.label, test_size=0.1)\n",
    "dataset_size = len(data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "#train_indices is equivalent to list(tr.index)\n",
    "#val_indices is equivalent to list(val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "Transforms are common image transformations. They can be chained together using **Compose**.\n",
    "## Normalization\n",
    "Normalize a tensor image with mean and standard deviation. Given mean: (M1,...,Mn) and std: (S1,..,Sn) for n channels, this transform will normalize each channel of the input torch.*Tensor i.e. input[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "Convert a PIL Image or numpy.ndarray to tensor.\n",
    "\n",
    "## ToTensor\n",
    "Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Create custom dataset class\n",
    " A dataset must contain following functions to be used by data loader later on.\n",
    "\n",
    "* __init__() function is where the initial logic happens like reading a csv, assigning transforms etc.\n",
    "* __getitem__() function returns the data and labels. This function is called from dataloader like this:\n",
    "\n",
    "> img, label = MyCustomDataset.__getitem__(99)  # For 99th item\n",
    "\n",
    "<br>\n",
    "An important thing to note is that __getitem__() return a specific type for a single data point (like a tensor, numpy array etc.), otherwise, in the data loader you will get an error like:\n",
    "\n",
    "> TypeError: batch must contain tensors, numbers, dicts or lists; found \n",
    "> class 'PIL.PngImagePlugin.PngImageFile'\n",
    "\n",
    "<br>\n",
    "Credits: [PyTorch Custom Dataset Examples](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arthopod_Dataset(Dataset):\n",
    "    def __init__(self, img_data,img_path,transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.transform = transform\n",
    "        self.img_data = img_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_name = os.path.join(self.img_path,self.img_data.loc[index, 'labels'],\n",
    "                                self.img_data.loc[index, 'Images'])\n",
    "        image = Image.open(img_name)\n",
    "        #image = image.convert('RGB')\n",
    "        image = image.resize((300,300))\n",
    "        label = torch.tensor(self.img_data.loc[index, 'encoded_labels'])\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** - In case you already have a separate dataset set for train and validation, you could directly pass the path of the datasets to the __init__ section of the custom dataset class and read it right there and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Arthopod_Dataset(data,BASE_PATH,transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_display(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    npimg = np.transpose(npimg, (1, 2, 0))\n",
    "    return npimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "visualizing the elements of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "arthopod_types = {0: 'Coleoptera', 1: 'Diptera', 2: 'Hymenoptera', 3: 'Lepidoptera'}\n",
    "# Viewing data examples used for training\n",
    "fig, axis = plt.subplots(3, 5, figsize=(15, 10))\n",
    "for i, ax in enumerate(axis.flat):\n",
    "    with torch.no_grad():\n",
    "        image, label = images[i], labels[i]\n",
    "        ax.imshow(img_display(image)) # add image\n",
    "        ax.set(title = f\"{arthopod_types[label.item()]}\") # add label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network\n",
    "In the **Net** class created below, we have constructed a neural network. Construction of the neural network was the second most difficult situation that I faced after constructing a custom dataset. But I am going to explain you everything step by step.<br>\n",
    "* Inside the **init()** method you declare each layer with a unique layer name. For every unique layer, declaring its input features and output features is a must. At least the input feature is a must for some of the layers like batch normalization.\n",
    "* Inside the **forward(self, x)** method you need to connect the layers that were declared in the init method. One thing must be kept in mind that the output feature of one layer is an input feature of its next connecting layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP** - If you are facing difficulty in constructing a neural network class, then I hope this tip would be very much helpful.<br><br>\n",
    "Before laying your hands on PyTorch I hope you would be familiar with **Keras**( another python library for deep-learning ). And if you are not, then please go practice Keras first as it allows the simplest way of implementing a deep learning model in python.<br><br>\n",
    "In Keras after implementing a neural network with Model API, there is a command called \"Model.summary()\" which gives you the entire structure of the neural network that you have created along with the number of parameters. Keep that summary side by side while constructing your network class in PyTorch, things would get much easier. It would be much easier to see that network summary, name the layers according to it and connect them concerning their input and output features in PyTorch. It was helpful in my case, and I hope It would be helpful to you also.\n",
    "![image](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQylY4AqnlZcpGxTSj7h8tG5ZeUESvLa9MLk51PDDHwkACgirUc)\n",
    "<br>Link to the model whose summary I used to construct my PyTorch model - [ Classifying Cursive hiragana(Â¥©„ÅóÂ≠ó) KMNIST using CNN](https://www.kaggle.com/gpreda/classifying-cursive-hiragana-kmnist-using-cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 3 input image channel, 16 output channels, 3x3 square convolution kernel\n",
    "        self.conv1 = nn.Conv2d(3,16,kernel_size=3,stride=2,padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32,kernel_size=3,stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64,kernel_size=3,stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 64,kernel_size=3,stride=2, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout2d(0.4)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64*5*5,512 )\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm1(F.relu(self.conv1(x)))\n",
    "        x = self.batchnorm2(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(self.batchnorm2(self.pool(x)))\n",
    "        x = self.batchnorm3(self.pool(F.relu(self.conv3(x))))\n",
    "        x = self.dropout(self.conv4(x))\n",
    "        x = x.view(-1, 64*5*5) # Flatten layer\n",
    "        x = self.dropout(self.fc1(x))\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x),dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net() # On CPU\n",
    "#model = Net().to(device)  # On GPU\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossEntropyLoss\n",
    "It is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set. It is a prototpe of categorical crossentropy in keras.\n",
    "In case of **Binary classification** use **BCELoss(Binary Cross Entropy)** or BCEWithLogitsLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    _,pred = torch.max(out, dim=1)\n",
    "    return torch.sum(pred==labels).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "In the cell below, it is explained how to train your model with epochs. In the \"train_loss\" and \"val_loss\" the training loss and validation loss are stored respectively after every epoch. Similarly in case of training accuracy and validation accuracy also, the same thing happens. just remember while validation the weighhts are not upgraded thats why we use **\" with torch.no_grad() \"**.<br>\n",
    "Here, **\"Torch.max(x, dim=1)\"** works same as **\"np.argmax(x, axis=1)\"**. We use **\".item()\"** to get the value inside the tensor. **torch.save(model.state_dict(), 'model_classification_tutorial.pt')** is used to save the PyTorch weight in the given directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 12\n",
    "print_every = 10\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    # scheduler.step(epoch)\n",
    "    correct = 0\n",
    "    total=0\n",
    "    print(f'Epoch {epoch}\\n')\n",
    "    for batch_idx, (data_, target_) in enumerate(train_loader):\n",
    "        #data_, target_ = data_.to(device), target_.to(device)# on GPU\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(data_)\n",
    "        loss = criterion(outputs, target_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        _,pred = torch.max(outputs, dim=1)\n",
    "        correct += torch.sum(pred==target_).item()\n",
    "        total += target_.size(0)\n",
    "        if (batch_idx) % 20 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain loss: {np.mean(train_loss):.4f}, train acc: {(100 * correct / total):.4f}')\n",
    "    batch_loss = 0\n",
    "    total_t=0\n",
    "    correct_t=0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data_t, target_t in (validation_loader):\n",
    "            #data_t, target_t = data_t.to(device), target_t.to(device)# on GPU\n",
    "            outputs_t = model(data_t)\n",
    "            loss_t = criterion(outputs_t, target_t)\n",
    "            batch_loss += loss_t.item()\n",
    "            _,pred_t = torch.max(outputs_t, dim=1)\n",
    "            correct_t += torch.sum(pred_t==target_t).item()\n",
    "            total_t += target_t.size(0)\n",
    "        val_acc.append(100 * correct_t / total_t)\n",
    "        val_loss.append(batch_loss/len(validation_loader))\n",
    "        network_learned = batch_loss < valid_loss_min\n",
    "        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t / total_t):.4f}\\n')\n",
    "        # Saving the best weight \n",
    "        if network_learned:\n",
    "            valid_loss_min = batch_loss\n",
    "            torch.save(model.state_dict(), 'model_classification_tutorial.pt')\n",
    "            print('Detected network improvement, saving current model')\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy and loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.title(\"Train - Validation Loss\")\n",
    "plt.plot( train_loss, label='train')\n",
    "plt.plot( val_loss, label='validation')\n",
    "plt.xlabel('num_epochs', fontsize=12)\n",
    "plt.ylabel('loss', fontsize=12)\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.title(\"Train - Validation Accuracy\")\n",
    "plt.plot(train_acc, label='train')\n",
    "plt.plot(val_acc, label='validation')\n",
    "plt.xlabel('num_epochs', fontsize=12)\n",
    "plt.ylabel('accuracy', fontsize=12)\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing trained Network with better loss of validation\n",
    "model.load_state_dict(torch.load('model_classification_tutorial.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "evaluating the model performance through visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(validation_loader)\n",
    "images, labels = dataiter.next()\n",
    "arthopod_types = {0: 'Coleoptera', 1: 'Diptera', 2: 'Hymenoptera', 3: 'Lepidoptera'}\n",
    "# Viewing data examples used for training\n",
    "fig, axis = plt.subplots(3, 5, figsize=(15, 10))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for ax, image, label in zip(axis.flat,images, labels):\n",
    "        ax.imshow(img_display(image)) # add image\n",
    "        image_tensor = image.unsqueeze_(0)\n",
    "        output_ = model(image_tensor)\n",
    "        output_ = output_.argmax()\n",
    "        k = output_.item()==label.item()\n",
    "        ax.set_title(str(arthopod_types[label.item()])+\":\" +str(k)) # add label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Pytorch?\n",
    "### (according to [PyTorch.org](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py))\n",
    "It‚Äôs a Python-based scientific computing package targeted at two sets of audiences:\n",
    "* A replacement for NumPy to use the power of GPUs\n",
    "* a deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "### (according to ME üò§)\n",
    "\n",
    "It is a python based library, useful for a well-documented program in deep learning. I did not observe much speed up in training due to the use of GPU, maybe with multiple GPUs, it might speed up its training. Due to its well-documented feature, I would agree that it is flexible.<br>\n",
    "I would recommend you to learn PyTorch but after having a hand full of experience in Keras or at least some knowledge about Keras. Pytorch should not be learned right at the get-go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Would there be any improvement in model performence while classification by using Pytorch?\n",
    "As far as I have learned about PyTorch, the answer is no. The same model architecture would give the same performance in Keras. If you find any exception please let me know in the comments down below. I would be happy to learn about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Whatever I have explained in this kernel is according to my personal experience in PyTorch as a beginner. If I have missed out to explain something then please let me know. I would be adding more chapters to this pytorch tutorial series. Till then please **UPVOTE** this kernel if you like it, feel free to let me know what all improvements that could be added to this tutorial and have a nice day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "* [PyTorch CNN from scratch](https://www.kaggle.com/bonhart/pytorch-cnn-from-scratch)\n",
    "* [Starting Kit for PyTorch Deep Learning](https://www.kaggle.com/mratsim/starting-kit-for-pytorch-deep-learning)\n",
    "* [Simple EDA and model in pytorch](https://www.kaggle.com/artgor/simple-eda-and-model-in-pytorch)\n",
    "* [CNN - Digit Recognizer (PyTorch)](https://www.kaggle.com/gustafsilva/cnn-digit-recognizer-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
