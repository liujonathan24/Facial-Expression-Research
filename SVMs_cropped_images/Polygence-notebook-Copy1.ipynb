{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dlib\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import local_binary_pattern\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"../shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_cropped_image(orig_path, new_path=None):\n",
    "    print(\"Processing file: {}\".format(orig_path))\n",
    "    img = dlib.load_rgb_image(orig_path)\n",
    "    face_area = detector(img, 1)\n",
    "    shape = predictor(img, face_area[0])\n",
    "    pts = np.array( # the 27 points on the face\n",
    "        [[shape.part(0).x, shape.part(0).y], [shape.part(3).x, shape.part(3).y],\n",
    "         [shape.part(4).x, shape.part(4).y], [shape.part(5).x, shape.part(5).y], \n",
    "         [shape.part(6).x, shape.part(6).y], [shape.part(7).x, shape.part(7).y],\n",
    "         [shape.part(8).x, shape.part(8).y], [shape.part(9).x, shape.part(9).y],\n",
    "         [shape.part(10).x, shape.part(10).y], [shape.part(11).x, shape.part(11).y], \n",
    "         [shape.part(12).x, shape.part(12).y], [shape.part(13).x, shape.part(13).y],\n",
    "         [shape.part(14).x, shape.part(14).y], [shape.part(15).x, shape.part(15).y],\n",
    "         [shape.part(16).x, shape.part(16).y], [shape.part(26).x, shape.part(26).y], \n",
    "         [shape.part(25).x, shape.part(25).y], [shape.part(24).x, shape.part(24).y],\n",
    "         [shape.part(23).x, shape.part(23).y], [shape.part(22).x, shape.part(22).y],\n",
    "         [shape.part(27).x, shape.part(27).y], [shape.part(21).x, shape.part(21).y], \n",
    "         [shape.part(20).x, shape.part(20).y], [shape.part(19).x, shape.part(19).y],\n",
    "         [shape.part(18).x, shape.part(18).y], [shape.part(17).x, shape.part(17).y]], \n",
    "         dtype=np.int32)\n",
    "    # Create polygon shaped mask\n",
    "    mask = np.zeros((img.shape[0], img.shape[1]), dtype=np.int32)\n",
    "    cv2.fillPoly(mask, np.int32([pts]), 1)\n",
    "    mask = mask.astype(bool)\n",
    "    # Fill in polygon with image\n",
    "    out = np.zeros_like(img)\n",
    "    out[mask] = img[mask]\n",
    "\n",
    "    #src = cv2.imread(f, 0)\n",
    "    cropped_image = out[int(face_area[0].top()):int(face_area[0].bottom()), int(face_area[0].left()):int(face_area[0].right())]\n",
    "    cropped_image = cv2.resize(cropped_image, (130, 130))\n",
    "    \n",
    "    #plt.imshow(cropped_image)\n",
    "    #plt.show()\n",
    "    cv2.imwrite(new_path, cropped_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_cropped_image(\"../Ck+/anger/S503_001_00000071.png\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a folder\n",
    "def preprocess_folder_of_emotions(folder_path, emotions_list, pic_type=\"*.png\"):\n",
    "    write_to = folder_path + \"_new\"\n",
    "    for emotion in emotions_list:\n",
    "        #print(\"first for loop\")\n",
    "        #print(pic_type)\n",
    "        emo_folder_path = folder_path + \"/\" + emotion\n",
    "        write_path = write_to + \"/\" + emotion\n",
    "        #print(\"elp_0\")\n",
    "        for pic_file in glob.glob(os.path.join(emo_folder_path, pic_type)):\n",
    "            #print(\"elp\")\n",
    "            #print(\"Writing file: {}\".format(pic_file))\n",
    "            write_pic_path = write_path + pic_file[-22:]\n",
    "            write_cropped_image(pic_file, write_pic_path)\n",
    "\n",
    "                                  \n",
    "emotions_list = [\"anger\", \"contempt\", \"disgust\", \"fear\", \"happiness\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "preprocess_folder_of_emotions(\"../CK+\", emotions_list, \"*.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LBP Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d6/73t68jg509l9qqjp_k_6st940000gp/T/ipykernel_72974/1619009774.py:10: DeprecationWarning: `itemfreq` is deprecated!\n",
      "`itemfreq` is deprecated and will be removed in a future version. Use instead `np.unique(..., return_counts=True)`\n",
      "  histogram = scipy.stats.itemfreq(lbp_image)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "# LBP\n",
    "emotions_list = [\"anger\", \"contempt\", \"disgust\", \"fear\", \"happiness\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "list_of_vecs = []\n",
    "counter = 0\n",
    "for a in emotions_list:\n",
    "    faces_folder_path = \"../CK+_new/\" + str(a)\n",
    "    for pic_file in glob.glob(os.path.join(faces_folder_path, \"*.png\")):\n",
    "        src = cv2.imread(pic_file, 0)\n",
    "        lbp_image = local_binary_pattern(src, 8, 2, method='nri_uniform')\n",
    "        histogram = scipy.stats.itemfreq(lbp_image)\n",
    "        vector = []\n",
    "        for c, b in histogram:\n",
    "            vector.append(b)\n",
    "        \n",
    "        list_of_vecs.append([counter, a, vector])\n",
    "        counter +=1\n",
    "print(len(list_of_vecs[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 'anger', [272.0, 179.0, 11.0, 50.0, 29.0, 199.0, 21.0, 66.0, 16.0, 125.0, 77.0, 38.0, 59.0, 97.0, 103.0, 36.0, 47.0, 286.0, 165.0, 173.0, 165.0, 213.0, 128.0, 231.0, 224.0, 698.0, 329.0, 267.0, 625.0, 596.0, 230.0, 209.0, 882.0, 338.0, 208.0, 216.0, 142.0, 283.0, 163.0, 141.0, 185.0, 61.0, 85.0, 115.0, 104.0, 42.0, 39.0, 89.0, 95.0, 39.0, 23.0, 59.0, 133.0, 31.0, 21.0, 42.0, 122.0, 6375.0, 903.0]], [1, 'anger', [170.0, 148.0, 9.0, 58.0, 13.0, 164.0, 10.0, 35.0, 17.0, 73.0, 49.0, 39.0, 30.0, 35.0, 76.0, 38.0, 35.0, 233.0, 65.0, 100.0, 88.0, 108.0, 27.0, 170.0, 117.0, 519.0, 517.0, 67.0, 360.0, 244.0, 98.0, 180.0, 468.0, 329.0, 158.0, 182.0, 77.0, 296.0, 70.0, 215.0, 117.0, 54.0, 57.0, 131.0, 80.0, 55.0, 60.0, 69.0, 104.0, 109.0, 26.0, 149.0, 200.0, 119.0, 31.0, 112.0, 142.0, 8089.0, 1509.0]]]\n"
     ]
    }
   ],
   "source": [
    "print(list_of_vecs[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORB Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for a in emotions_list:\n",
    "    faces_folder_path = \"../CK+_new/\" + a\n",
    "    for f in glob.glob(os.path.join(faces_folder_path, \"*.png\")):\n",
    "        #print(\"Processing file: {}\".format(f))\n",
    "        src = cv2.imread(f, 0)\n",
    "        src = src[1:129, 1:129]\n",
    "        #print(src.shape)\n",
    "        #print(len(src[0]))\n",
    "        #print('hi')\n",
    "        # Initiate ORB detector\n",
    "        orb = cv2.ORB_create()\n",
    "        # find the keypoints with ORB\n",
    "        kp = orb.detect(src, None)\n",
    "        # compute the descriptors with ORB\n",
    "        kp, des = orb.compute(src, kp)\n",
    "        #print(des[0])\n",
    "        #print(des[0].shape)\n",
    "        #print(des[0].tolist())\n",
    "        list_of_vecs[counter].append(des[0].tolist())\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 'anger', [272.0, 179.0, 11.0, 50.0, 29.0, 199.0, 21.0, 66.0, 16.0, 125.0, 77.0, 38.0, 59.0, 97.0, 103.0, 36.0, 47.0, 286.0, 165.0, 173.0, 165.0, 213.0, 128.0, 231.0, 224.0, 698.0, 329.0, 267.0, 625.0, 596.0, 230.0, 209.0, 882.0, 338.0, 208.0, 216.0, 142.0, 283.0, 163.0, 141.0, 185.0, 61.0, 85.0, 115.0, 104.0, 42.0, 39.0, 89.0, 95.0, 39.0, 23.0, 59.0, 133.0, 31.0, 21.0, 42.0, 122.0, 6375.0, 903.0], [59, 34, 68, 1, 67, 90, 96, 64, 117, 100, 230, 171, 75, 12, 150, 212, 120, 140, 234, 132, 13, 178, 229, 128, 22, 96, 128, 3, 103, 140, 94, 196]], [1, 'anger', [170.0, 148.0, 9.0, 58.0, 13.0, 164.0, 10.0, 35.0, 17.0, 73.0, 49.0, 39.0, 30.0, 35.0, 76.0, 38.0, 35.0, 233.0, 65.0, 100.0, 88.0, 108.0, 27.0, 170.0, 117.0, 519.0, 517.0, 67.0, 360.0, 244.0, 98.0, 180.0, 468.0, 329.0, 158.0, 182.0, 77.0, 296.0, 70.0, 215.0, 117.0, 54.0, 57.0, 131.0, 80.0, 55.0, 60.0, 69.0, 104.0, 109.0, 26.0, 149.0, 200.0, 119.0, 31.0, 112.0, 142.0, 8089.0, 1509.0], [11, 158, 246, 223, 175, 254, 188, 151, 191, 223, 150, 245, 140, 190, 214, 239, 135, 119, 255, 149, 181, 201, 55, 224, 15, 122, 222, 211, 142, 212, 141, 223]]]\n"
     ]
    }
   ],
   "source": [
    "print(list_of_vecs[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_fold_thingy(full_list):\n",
    "    overall_list = []\n",
    "    for val in full_list:\n",
    "        vector_LBP = val[2]\n",
    "        vector_ORB = val[3]\n",
    "        max_lbp = max(vector_LBP)\n",
    "        vector_LBP = [a/max_lbp for a in vector_LBP]\n",
    "\n",
    "        max_orb = max(vector_ORB)\n",
    "        vector_ORB = [a/max_orb for a in vector_ORB]\n",
    "        overall_list.append([val[1], vector_LBP+vector_ORB])\n",
    "\n",
    "    c = 1e-5\n",
    "    for counter, (emo, vector) in enumerate(overall_list):\n",
    "        #print(emo)\n",
    "        #print(vector)\n",
    "        avg = sum(vector)/len(vector)\n",
    "        avg2 = 0\n",
    "        r = 0\n",
    "        \n",
    "        for a in vector:\n",
    "            avg2 +=a/91\n",
    "        for a in vector:\n",
    "            r += (a-avg)**2\n",
    "        \n",
    "        new = []\n",
    "        for counter2, a in enumerate(vector):\n",
    "            #if counter < 1:\n",
    "            #    print(a)\n",
    "            #    print(val)\n",
    "            val = 100 * (a-avg)/(r+c)\n",
    "            \n",
    "            new.append(val)\n",
    "        #print(len(overall_list[counter][1]))\n",
    "        #print(len(new))\n",
    "        overall_list[counter][1] = new\n",
    "    return overall_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', [-2.0754366074346406, -2.276046620809393, -2.63843890303475, -2.554312123232435, -2.5996111585106045, -2.232904682449231, -2.6168679338546688, -2.5197985725443055, -2.6276534184447096, -2.392529854381829, -2.4960705064462165, -2.580197286248532, -2.534898250970362, -2.4529285680860546, -2.439985986578007, -2.5845114800845477, -2.560783413986459, -2.0452372505825274, -2.3062459776615056, -2.2889892023174405, -2.3062459776615056, -2.2027053255971176, -2.3860585636278047, -2.1638775810729722, -2.1789772594990287, -1.1565133203631979, -1.95248208310818, -2.086222092024681, -1.3139813953777881, -1.3765372060000225, -2.16603467799098, -2.21133371326915, -0.7596074874497108, -1.9330682108461075, -2.213490810187158, -2.1962340348430933, -2.3558592067756914, -2.0517085413365517, -2.3105601714975212, -2.3580163036936996, -2.2631040393013437, -2.530584057134346, -2.4788137311021523, -2.4141008235619092, -2.437828889659998, -2.5715688985765, -2.5780401893305234, -2.4701853434301198, -2.4572427619220716, -2.5780401893305234, -2.612553740018653, -2.534898250970362, -2.3752730790377643, -2.5952969646745885, -2.6168679338546688, -2.5715688985765, -2.399001145135853, 11.08932588316868, -0.7143084521715415, 0.8050897756782279, -0.6640868111061222, 1.3339933469205945, -2.6033999056614645, 1.2752262834492198, 2.6268687432908226, 2.979471124119067, 1.0989250930350984, 4.213579457017921, 3.2145393780045626, 10.854257629283182, 7.3870008844721164, 1.7453627912202125, -1.956962207476351, 6.152892551573264, 9.796450486798452, 4.389880647432043, 5.565221916859523, 11.08932588316868, 5.095085409088531, -1.8981951440049767, 7.798370328771735, 10.79549056581181, 4.860017155203036, -1.3692915727626107, 2.979471124119067, 4.860017155203036, -2.485865778718717, 3.390840568418685, 5.565221916859523, 2.861936997176319, 8.856177471256467]]\n"
     ]
    }
   ],
   "source": [
    "print(z_fold_thingy(list_of_vecs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[571, 466, 210, 625, 848]\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(range(len(list_of_vecs)), 5))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_2(train_percent, trials, list_of_vecs):\n",
    "    emo_dict = {\"anger\": 0, \"contempt\": 1, \"disgust\": 2, \"fear\": 3, \"happiness\": 4, \"neutral\": 5, \"sadness\": 6,\n",
    "            \"surprise\": 7}\n",
    "    #print(list_of_vecs)\n",
    "    shuffle_desk = [a for a in list_of_vecs]\n",
    "    \n",
    "    #print(\"1\")\n",
    "    list_of_emotion_of_vectors = {} # Creates a dictionary\n",
    "    #print(shuffle_desk)\n",
    "    for [emo,vector] in shuffle_desk:\n",
    "        #print(emo)\n",
    "        if emo not in list_of_emotion_of_vectors.keys():\n",
    "            list_of_emotion_of_vectors[emo] = [vector]\n",
    "        else:\n",
    "            list_of_emotion_of_vectors[emo].append(vector)\n",
    "          \n",
    "    #print(shuffle_desk[0])\n",
    "    #print(len(shuffle_desk))\n",
    "    #train_count = int((len(list_of_vecs) * (train_percent/100))//1) \n",
    "    #print(train_count)\n",
    "    #print(\"before for loop\")\n",
    "    for a in range(trials):\n",
    "        train_vectors = []\n",
    "        train_labels = []\n",
    "        test_vectors = []\n",
    "        test_labels = []\n",
    "        #print(list_of_emotion_of_vectors)\n",
    "        for emotion in list_of_emotion_of_vectors.keys():\n",
    "            #print(\"Keys:\")\n",
    "            #print(emotion)\n",
    "            length = len(list_of_emotion_of_vectors[emotion])\n",
    "            chosen = int(math.ceil(length * (train_percent/100))) # round down\n",
    "            #print(\"chosen\")\n",
    "            #print(chosen)\n",
    "            #print(length)\n",
    "            if length > 100:\n",
    "                for vector in list_of_emotion_of_vectors[emotion][:train_percent]:\n",
    "                    #print(\"train\")\n",
    "                    train_vectors.append(vector)\n",
    "                    train_labels.append(emo_dict[emotion])\n",
    "                for vector in list_of_emotion_of_vectors[emotion][train_percent:100]:\n",
    "                    #print(\"test\")\n",
    "                    #print(emo)\n",
    "                    test_vectors.append(vector)\n",
    "                    test_labels.append(emo_dict[emotion])\n",
    "            else:\n",
    "                for vector in list_of_emotion_of_vectors[emotion][:chosen]:\n",
    "                    #print(\"train\")\n",
    "                    train_vectors.append(vector)\n",
    "                    train_labels.append(emo_dict[emotion])\n",
    "                for vector in list_of_emotion_of_vectors[emotion][chosen:]:\n",
    "                    #print(\"test\")\n",
    "                    #print(emo)\n",
    "                    test_vectors.append(vector)\n",
    "                    test_labels.append(emo_dict[emotion])\n",
    "        #print(test_labels)\n",
    "    \n",
    "         \n",
    "        # Train the SVM\n",
    "#        svm_object = svm.SVC(kernel='rbf', gamma=100, C=1e-6) # 64 with normal set\n",
    "\n",
    "        svm_object = svm.SVC(kernel='rbf', gamma=\"auto\")        \n",
    "\n",
    "        print(test_labels)\n",
    "        svm_object.fit(np.asarray(train_vectors), train_labels)\n",
    "\n",
    "        \n",
    "        # Test it\n",
    "        # Confusion matrix: column headers = correct, rows = what was outputted\n",
    "        confusion_matrix_count = np.zeros((8,8))\n",
    "        right = 0\n",
    "        wrong = 0\n",
    "        #print(len(test_labels))\n",
    "        for label, vector in zip(test_labels, np.asarray(test_vectors)):\n",
    "            response = svm_object.predict([vector])\n",
    "            confusion_matrix_count[int(response)][label] += 1\n",
    "            #print(response)\n",
    "            if response == label:\n",
    "                right += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        print(right/(right + wrong) * 100)\n",
    "        print(confusion_matrix_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 2, 2, 3, 4, 4, 4, 5, 5, 5, 5, 5, 6, 7, 7, 7, 7]\n",
      "27.77777777777778\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 2. 3. 1. 2.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 2. 0. 2.]]\n"
     ]
    }
   ],
   "source": [
    "thingy = z_fold_thingy(list_of_vecs)\n",
    "train_svm_2(95, 1, thingy)\n",
    "\n",
    "# With train data:\n",
    "#83.48519362186788\n",
    "#[[ 15.   0.   0.   0.   0.   0.   0.   0.]\n",
    "# [  0.   3.   0.   0.   0.   0.   0.   0.]\n",
    "# [  0.   0.  35.   0.   0.   0.   0.   0.]\n",
    "# [  0.   0.   0.  11.   0.   0.   0.   0.]\n",
    "# [  0.   0.   0.   0.  36.   0.   0.   0.]\n",
    "# [ 28.  15.  22.  13.  30. 564.  19.  18.]\n",
    "# [  0.   0.   0.   0.   0.   0.   8.   0.]\n",
    "# [  0.   0.   0.   0.   0.   0.   0.  61.]]\n",
    "\n",
    "# With test data: 69.04761904761905\n",
    "#[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "# [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "# [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "# [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "# [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "# [ 2.  0.  2.  1.  3. 29.  1.  4.]\n",
    "# [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "# [ 0.  0.  0.  0.  0.  0.  0.  0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_3(shift, trials, list_of_vecs):\n",
    "    emo_dict = {\"anger\": 0, \"contempt\": 1, \"disgust\": 2, \"fear\": 3, \"happiness\": 4, \"neutral\": 5, \"sadness\": 6,\n",
    "            \"surprise\": 7}\n",
    "    #print(list_of_vecs)\n",
    "    shuffled_desk = [a for a in list_of_vecs]\n",
    "    label_emotions = []\n",
    "    data_vector_list = []\n",
    "    test_emo = []\n",
    "    test_vector = []\n",
    "\n",
    "    for a in range(920):\n",
    "        print(\"hi\")\n",
    "        # exclude 5% of each?\n",
    "        # emo_dict_count = {'anger': 45, 'contempt': 18, 'disgust': 59, 'fear': 25, 'happiness': 69, 'neutral': 593,\n",
    "        # 'sadness': 28, 'surprise': 83}\n",
    "        # Maybe used F-1 score\n",
    "        # Something which accounts for the imbalanced counts on each thing\n",
    "        if a in list(range(-shift, 920, 20)):\n",
    "            emotion, vector = shuffled_desk[a]\n",
    "            test_emo.append(emo_dict[emotion])\n",
    "            test_vector.append(vector)\n",
    "            continue\n",
    "        emotion, vector = shuffled_desk[a]\n",
    "        data_vector_list.append(vector)\n",
    "        label_emotions.append(emo_dict[emotion])  \n",
    "    \n",
    "        labels = np.asarray(label_emotions, dtype=int)\n",
    "        trainingData = np.asarray(data_vector_list, dtype=np.float32)\n",
    "\n",
    "        testEmo = np.asarray(test_emo, dtype=int)\n",
    "        testData = np.asarray(test_vector, dtype=np.float32)\n",
    "    \n",
    "        # Train the SVM\n",
    "        svm_object = svm.NuSVC(kernel=\"rbf\", gamma=\"auto\")\n",
    "        print(labels)\n",
    "        svm_object.fit(trainingData, labels)\n",
    "        \n",
    "        # Test it\n",
    "        # Confusion matrix: column headers = correct, rows = what was outputted\n",
    "        confusion_matrix_count = np.zeros((8,8))\n",
    "        right = 0\n",
    "        wrong = 0\n",
    "        #print(len(test_labels))\n",
    "        for label, vector in zip(testEmo, testData):\n",
    "            response = svm_object.predict([vector])\n",
    "            confusion_matrix_count[int(response)][label] += 1\n",
    "            print(response)\n",
    "            if response == label:\n",
    "                right += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        print(right/(right + wrong) * 100)\n",
    "        print(confusion_matrix_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n",
      "[0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of classes has to be greater than one; got 1 class",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [116]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m thingy \u001b[38;5;241m=\u001b[39m z_fold_thingy(list_of_vecs)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_svm_3\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthingy\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [115]\u001b[0m, in \u001b[0;36mtrain_svm_3\u001b[0;34m(shift, trials, list_of_vecs)\u001b[0m\n\u001b[1;32m     34\u001b[0m svm_object \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mSVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels)\n\u001b[0;32m---> 36\u001b[0m \u001b[43msvm_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainingData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Test it\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Confusion matrix: column headers = correct, rows = what was outputted\u001b[39;00m\n\u001b[1;32m     40\u001b[0m confusion_matrix_count \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/PyCharm/lib/python3.9/site-packages/sklearn/svm/_base.py:199\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    191\u001b[0m         X,\n\u001b[1;32m    192\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    197\u001b[0m     )\n\u001b[0;32m--> 199\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m    202\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[1;32m    203\u001b[0m )\n\u001b[1;32m    204\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m LIBSVM_IMPL\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/PyCharm/lib/python3.9/site-packages/sklearn/svm/_base.py:720\u001b[0m, in \u001b[0;36mBaseSVC._validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight_ \u001b[38;5;241m=\u001b[39m compute_class_weight(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight, classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, y\u001b[38;5;241m=\u001b[39my_)\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of classes has to be greater than one; got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m class\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    723\u001b[0m     )\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
     ]
    }
   ],
   "source": [
    "thingy = z_fold_thingy(list_of_vecs)\n",
    "train_svm_3(0, 1, thingy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
