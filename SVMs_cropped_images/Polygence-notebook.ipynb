{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6088ae3-6924-4ee9-a0e4-f2d78c9b6118",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "32a27d23-8d01-4cfc-a102-aa75a1ebab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dlib\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import local_binary_pattern\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583541a1-e160-4e9b-a3f6-2d9af8697376",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755b591e-b3c5-4dd0-97e3-f82094890bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"../shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf6006d2-78c7-4359-9d40-6883fa4a9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_cropped_image(orig_path, new_path=None):\n",
    "    print(\"Processing file: {}\".format(orig_path))\n",
    "    img = dlib.load_rgb_image(orig_path)\n",
    "    face_area = detector(img, 1)\n",
    "    shape = predictor(img, face_area[0])\n",
    "    pts = np.array( # the 27 points on the face\n",
    "        [[shape.part(0).x, shape.part(0).y], [shape.part(3).x, shape.part(3).y],\n",
    "         [shape.part(4).x, shape.part(4).y], [shape.part(5).x, shape.part(5).y], \n",
    "         [shape.part(6).x, shape.part(6).y], [shape.part(7).x, shape.part(7).y],\n",
    "         [shape.part(8).x, shape.part(8).y], [shape.part(9).x, shape.part(9).y],\n",
    "         [shape.part(10).x, shape.part(10).y], [shape.part(11).x, shape.part(11).y], \n",
    "         [shape.part(12).x, shape.part(12).y], [shape.part(13).x, shape.part(13).y],\n",
    "         [shape.part(14).x, shape.part(14).y], [shape.part(15).x, shape.part(15).y],\n",
    "         [shape.part(16).x, shape.part(16).y], [shape.part(26).x, shape.part(26).y], \n",
    "         [shape.part(25).x, shape.part(25).y], [shape.part(24).x, shape.part(24).y],\n",
    "         [shape.part(23).x, shape.part(23).y], [shape.part(22).x, shape.part(22).y],\n",
    "         [shape.part(27).x, shape.part(27).y], [shape.part(21).x, shape.part(21).y], \n",
    "         [shape.part(20).x, shape.part(20).y], [shape.part(19).x, shape.part(19).y],\n",
    "         [shape.part(18).x, shape.part(18).y], [shape.part(17).x, shape.part(17).y]], \n",
    "         dtype=np.int32)\n",
    "    # Create polygon shaped mask\n",
    "    mask = np.zeros((img.shape[0], img.shape[1]), dtype=np.int32)\n",
    "    cv2.fillPoly(mask, np.int32([pts]), 1)\n",
    "    mask = mask.astype(bool)\n",
    "    # Fill in polygon with image\n",
    "    out = np.zeros_like(img)\n",
    "    out[mask] = img[mask]\n",
    "\n",
    "    #src = cv2.imread(f, 0)\n",
    "    cropped_image = out[int(face_area[0].top()):int(face_area[0].bottom()), int(face_area[0].left()):int(face_area[0].right())]\n",
    "    cropped_image = cv2.resize(cropped_image, (130, 130))\n",
    "    \n",
    "    #plt.imshow(cropped_image)\n",
    "    #plt.show()\n",
    "    cv2.imwrite(new_path, cropped_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed1a53-f28c-4222-891a-f9d7671aa61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_cropped_image(\"../Ck+/anger/S503_001_00000071.png\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786a7b5-4402-4ad8-b5c5-1d5ed2f5826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a folder\n",
    "def preprocess_folder_of_emotions(folder_path, emotions_list, pic_type=\"*.png\"):\n",
    "    write_to = folder_path + \"_new\"\n",
    "    for emotion in emotions_list:\n",
    "        #print(\"first for loop\")\n",
    "        #print(pic_type)\n",
    "        emo_folder_path = folder_path + \"/\" + emotion\n",
    "        write_path = write_to + \"/\" + emotion\n",
    "        #print(\"elp_0\")\n",
    "        for pic_file in glob.glob(os.path.join(emo_folder_path, pic_type)):\n",
    "            #print(\"elp\")\n",
    "            #print(\"Writing file: {}\".format(pic_file))\n",
    "            write_pic_path = write_path + pic_file[-22:]\n",
    "            write_cropped_image(pic_file, write_pic_path)\n",
    "\n",
    "                                  \n",
    "emotions_list = [\"anger\", \"contempt\", \"disgust\", \"fear\", \"happiness\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "preprocess_folder_of_emotions(\"../CK+\", emotions_list, \"*.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045de533-2c8e-4b39-b7af-decc3546e57f",
   "metadata": {},
   "source": [
    "## LBP Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8bc0caa-f106-412a-a1ec-72a960e3ccf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d6/73t68jg509l9qqjp_k_6st940000gp/T/ipykernel_48744/3920418892.py:10: DeprecationWarning: `itemfreq` is deprecated!\n",
      "`itemfreq` is deprecated and will be removed in a future version. Use instead `np.unique(..., return_counts=True)`\n",
      "  histogram = scipy.stats.itemfreq(lbp_image)\n"
     ]
    }
   ],
   "source": [
    "# LBP\n",
    "emotions_list = [\"anger\", \"contempt\", \"disgust\", \"fear\", \"happiness\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "list_of_vecs = []\n",
    "counter = 0\n",
    "for a in emotions_list:\n",
    "    faces_folder_path = \"../CK+_new/\" + str(a)\n",
    "    for pic_file in glob.glob(os.path.join(faces_folder_path, \"*.png\")):\n",
    "        src = cv2.imread(pic_file, 0)\n",
    "        lbp_image = local_binary_pattern(src, 8, 2, method='nri_uniform')\n",
    "        histogram = scipy.stats.itemfreq(lbp_image)\n",
    "        vector = []\n",
    "        for c, b in histogram:\n",
    "            vector.append(b)\n",
    "        \n",
    "        list_of_vecs.append([counter, a, vector])\n",
    "        counter +=1\n",
    "#print(len(list_of_vecs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7130150e-6162-4898-8157-ef49b75a2120",
   "metadata": {},
   "source": [
    "## ORB Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e72fed9a-ad45-4701-a803-8f1168195802",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for a in emotions_list:\n",
    "    faces_folder_path = \"../CK+_new/\" + a\n",
    "    for f in glob.glob(os.path.join(faces_folder_path, \"*.png\")):\n",
    "        #print(\"Processing file: {}\".format(f))\n",
    "        src = cv2.imread(f, 0)\n",
    "        src = src[1:129, 1:129]\n",
    "        #print(src.shape)\n",
    "        #print(len(src[0]))\n",
    "        #print('hi')\n",
    "        # Initiate ORB detector\n",
    "        orb = cv2.ORB_create()\n",
    "        # find the keypoints with ORB\n",
    "        kp = orb.detect(src, None)\n",
    "        # compute the descriptors with ORB\n",
    "        kp, des = orb.compute(src, kp)\n",
    "        #print(des[0])\n",
    "        #print(des[0].shape)\n",
    "        #print(des[0].tolist())\n",
    "        list_of_vecs[counter].append(des[0].tolist())\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f0b11c8-0a21-4dd8-a91c-087be87ec44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_vecs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d83d5-fea8-4587-85a8-dcf040f8006e",
   "metadata": {},
   "source": [
    "## Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30d6796e-d520-4017-a0a4-002f31b31b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_fold_thingy(full_list):\n",
    "    overall_list = []\n",
    "    for val in full_list:\n",
    "        vector_LBP = val[2]\n",
    "        vector_ORB = val[3]\n",
    "        max_lbp = max(vector_LBP)\n",
    "        vector_LBP = [a/max_lbp for a in vector_LBP]\n",
    "\n",
    "        max_orb = max(vector_ORB)\n",
    "        vector_ORB = [a / max_orb for a in vector_ORB]\n",
    "        overall_list.append([val[1], vector_LBP+vector_ORB])\n",
    "\n",
    "    c = 1e-5\n",
    "    for counter, (emo, vector) in enumerate(overall_list):\n",
    "        avg = sum(vector)/len(vector)\n",
    "        r = 0\n",
    "        for a in vector:\n",
    "            r += (a-avg)**2\n",
    "        \n",
    "        new = []\n",
    "        for counter, a in enumerate(vector):\n",
    "            new.append(100 * (a-avg)/(r+c))\n",
    "        overall_list[counter][1] = new\n",
    "    return overall_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14104283-fca8-4ada-9654-8e8cd0accf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "print(len(z_fold_thingy(list_of_vecs)[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "60578682-7b7b-46e4-a0a5-e53a1a3d0879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(train_percent, trials, list_of_vecs):\n",
    "    emo_dict = {\"anger\": 0, \"contempt\": 1, \"disgust\": 2, \"fear\": 3, \"happiness\": 4, \"neutral\": 5, \"sadness\": 6,\n",
    "            \"surprise\": 7}\n",
    "    shuffle_desk = [a for a in list_of_vecs]\n",
    "    \n",
    "    #random.shuffle(shuffle_desk)\n",
    "    #print(shuffle_desk[0])\n",
    "    #print(len(shuffle_desk))\n",
    "    train_count = int((len(list_of_vecs) * (train_percent/100))//1) # Round down\n",
    "    print(train_count)\n",
    "    \n",
    "    for trial_number in range(trials):\n",
    "        train_set = random.sample(range(len(list_of_vecs)), train_count)\n",
    "        train_labels, train_vectors = list(zip(*[shuffle_desk[a] for a in train_set]))\n",
    "        \n",
    "        train_vectors = np.matrix(train_vectors, dtype=np.float32)\n",
    "        train_labels = np.array([emo_dict[a] for a in train_labels], dtype=int)\n",
    "\n",
    "        test_set = [a for a in range(len(list_of_vecs)) if a not in train_set]\n",
    "        test_labels, test_vectors = list(zip(*[shuffle_desk[a] for a in test_set]))\n",
    "        \n",
    "        test_vectors = np.matrix(test_vectors, dtype=np.float32)\n",
    "        test_labels = np.array([emo_dict[a] for a in test_labels], dtype=int)\n",
    "        \n",
    "        # Repeat until every emotion is in test_labels\n",
    "        if 0 not in test_labels or 1 not in test_labels or 2 not in test_labels or 3 not in test_labels or 4 not in test_labels or 5 not in test_labels or 6 not in test_labels or 7 not in test_labels:\n",
    "            print(\"hi\")\n",
    "            while 0 not in test_labels or 1 not in test_labels or 2 not in test_labels or 3 not in test_labels or 4 not in test_labels or 5 not in test_labels or 6 not in test_labels or 7 not in test_labels:\n",
    "                train_set = random.sample(range(len(list_of_vecs)), train_count)\n",
    "                train_labels, train_vectors = list(zip(*[shuffle_desk[a] for a in train_set]))\n",
    "\n",
    "                train_vectors = np.matrix(train_vectors, dtype=np.float32)\n",
    "                train_labels = np.array([emo_dict[a] for a in train_labels], dtype=int)\n",
    "\n",
    "\n",
    "                test_set = [a for a in range(len(list_of_vecs)) if a not in train_set]\n",
    "                test_labels, test_vectors = list(zip(*[shuffle_desk[a] for a in test_set]))\n",
    "\n",
    "                test_vectors = np.matrix(test_vectors, dtype=np.float32)\n",
    "                test_labels = np.array([emo_dict[a] for a in test_labels], dtype=int)\n",
    "                print(len(train_set))\n",
    "                print(len(test_set))\n",
    "        \n",
    "        # Train the SVM\n",
    "        svm = cv2.ml.SVM_create()\n",
    "        svm.setType(cv2.ml.SVM_C_SVC)\n",
    "        \n",
    "        svm.setKernel(cv2.ml.SVM_RBF)\n",
    "        svm.setTermCriteria((cv2.TERM_CRITERIA_MAX_ITER, 100, 1e-6))\n",
    "        print(train_vectors)\n",
    "        print(train_labels)\n",
    "        svm.train(train_vectors, cv2.ml.ROW_SAMPLE, train_labels)\n",
    "        \n",
    "        # Test it\n",
    "        # Confusion matrix: column headers = correct, rows = what was outputted\n",
    "        confusion_matrix_count = np.zeros((8,8))\n",
    "        right = 0\n",
    "        wrong = 0\n",
    "        print(len(test_labels))\n",
    "        for label, vector in zip(train_labels, train_vectors):\n",
    "            \n",
    "            response = svm.predict(vector)[1]\n",
    "            confusion_matrix_count[int(response)][label] += 1\n",
    "            if response == label:\n",
    "                right += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        print(right/(right + wrong) * 100)\n",
    "        print(confusion_matrix_count)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4c6ed-2de0-4710-9b4d-314e93cbd6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "thingy = z_fold_thingy(list_of_vecs)\n",
    "train_svm(95, 1, thingy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "844f1b84-f820-4886-896b-7fb81e54071c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[571, 466, 210, 625, 848]\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(range(len(list_of_vecs)), 5))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a297d89c-e62c-4303-95f1-00a2a703a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_2(train_percent, trials, list_of_vecs):\n",
    "    emo_dict = {\"anger\": 0, \"contempt\": 1, \"disgust\": 2, \"fear\": 3, \"happiness\": 4, \"neutral\": 5, \"sadness\": 6,\n",
    "            \"surprise\": 7}\n",
    "    shuffle_desk = [a for a in list_of_vecs]\n",
    "    \n",
    "    random.shuffle(shuffle_desk) # Shuffles desk\n",
    "    \n",
    "    past_emo = 0\n",
    "    list_of_emotion_of_vectors = {} # Creates a dictionary\n",
    "    for [emo,vector] in shuffle_desk:\n",
    "        if emo not in list_of_emotion_of_vectors.keys():\n",
    "            list_of_emotion_of_vectors[emo] = [vector]\n",
    "        else:\n",
    "            list_of_emotion_of_vectors[emo].append(vector)\n",
    "    print()\n",
    "          \n",
    "    #print(shuffle_desk[0])\n",
    "    #print(len(shuffle_desk))\n",
    "    #train_count = int((len(list_of_vecs) * (train_percent/100))//1) \n",
    "    #print(train_count)\n",
    "    \n",
    "    for a in range(trials):\n",
    "        train_vectors = []\n",
    "        train_labels = []\n",
    "        test_vectors = []\n",
    "        test_labels = []\n",
    "        for emotion in list_of_emotion_of_vectors.keys():\n",
    "            length = len(list_of_emotion_of_vectors[emotion])\n",
    "            print(length)\n",
    "            chosen = int(math.ceil(length * (train_percent/100))) # round down\n",
    "\n",
    "            for vector in list_of_emotion_of_vectors[emotion][:chosen]:\n",
    "                train_vectors.append(vector)\n",
    "                train_labels.append(emo_dict[emotion])\n",
    "            for vector in list_of_emotion_of_vectors[emotion][chosen:]:\n",
    "                test_vectors.append(vector)\n",
    "                test_labels.append(emo_dict[emotion])\n",
    "        print(test_labels[0])\n",
    "    \n",
    "         \n",
    "        # Train the SVM\n",
    "        svm = cv2.ml.SVM_create()\n",
    "        svm.setType(cv2.ml.SVM_C_SVC)\n",
    "        \n",
    "        svm.setKernel(cv2.ml.SVM_RBF)\n",
    "        svm.setTermCriteria((cv2.TERM_CRITERIA_MAX_ITER, 100, 1e-6))\n",
    "        #print(train_vectors)\n",
    "        #print(train_labels)\n",
    "        svm.train(np.matrix(train_vectors, dtype=np.float32), cv2.ml.ROW_SAMPLE, np.array(train_labels, dtype=int))\n",
    "        \n",
    "        # Test it\n",
    "        # Confusion matrix: column headers = correct, rows = what was outputted\n",
    "        confusion_matrix_count = np.zeros((8,8))\n",
    "        right = 0\n",
    "        wrong = 0\n",
    "        print(len(test_labels))\n",
    "        for label, vector in zip(test_labels, np.matrix(test_vectors, dtype=np.float32)):\n",
    "            \n",
    "            response = svm.predict(vector)[1]\n",
    "            confusion_matrix_count[int(response)][label] += 1\n",
    "            if response == label:\n",
    "                right += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        print(right/(right + wrong) * 100)\n",
    "        print(confusion_matrix_count)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fbc77a13-a5af-46fc-8d60-428c3f298b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "593\n",
      "83\n",
      "25\n",
      "18\n",
      "69\n",
      "59\n",
      "28\n",
      "45\n",
      "5\n",
      "42\n",
      "69.04761904761905\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  0.  2.  1.  3. 29.  1.  4.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "thingy = z_fold_thingy(list_of_vecs)\n",
    "train_svm_2(95, 1, thingy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2623d8a8-c53b-4828-9518-c0990c7451c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
